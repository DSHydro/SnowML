{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c7cd88-c6f2-49c0-a317-1ae3e07266d7",
   "metadata": {},
   "source": [
    "# Notebook to Investigate LSTM Performance Variance for Maritime Only Single Snow Type Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190667a-9b99-44c4-87a8-8d36f8bea340",
   "metadata": {},
   "source": [
    "# Step 0 - Set up Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef22fa2-0a8e-4982-bd09-bdb98e4d848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libaries\n",
    "import os\n",
    "import boto3\n",
    "import mlflow\n",
    "import time\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from snowML.viz import download_metrics as dm\n",
    "from snowML.datapipe.utils import snow_types as st\n",
    "from snowML.datapipe.utils import get_geos as gg\n",
    "from snowML.datapipe.utils import data_utils as du\n",
    "from snowML.datapipe.utils import get_dem as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97c2faf-fcda-42d0-9120-0eacc9073064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize erathengine credentials\n",
    "import ee\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06deed74-0fac-4b31-8687-14affeefeccc",
   "metadata": {},
   "source": [
    "# Step1 - Get MLFlow Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ca9c8c-1f0f-4ecb-9751-04ada581e82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dict = {} \n",
    "\n",
    "run_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f9cff8-9dd6-4f87-a22f-e7fe7268ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of dataframes corresponding to the run_id shortname uploading from local files \n",
    "# Assumes you have already run the \"download metrics\" script from the snowML.Scripts package to save the metrics to local files\n",
    "df_dict = {}\n",
    "for key in run_dict.keys(): \n",
    "    run_id = run_dict[key]\n",
    "    file = f\"../../mlflow_data/run_id_data/metrics_from_{run_id}.csv\"\n",
    "    df_metrics = pd.read_csv(file)\n",
    "    #print(df_metrics.shape)\n",
    "    df_dict[key] = df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53aaa6c-bf22-41b2-b217-c1f9af9cfc8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Mar_Hum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# display example\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#key = \"Elev_and_FC_30\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMar_Hum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df_metrics \u001b[38;5;241m=\u001b[39m df_dict[key]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_metrics\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m df_metrics\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Mar_Hum'"
     ]
    }
   ],
   "source": [
    "# display example\n",
    "#key = \"Elev_and_FC_30\"\n",
    "key = \"Mar_Hum\"\n",
    "df_metrics = df_dict[key]\n",
    "print(df_metrics.shape)\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb5bc6-4300-4903-a9bd-7ea99084fd65",
   "metadata": {},
   "source": [
    "# Step 2 - Functions to Summarize Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f849d-2653-497f-a35e-f218e4cd500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract only a specific metric by suffix\n",
    "def extract_metric_by_suffix(df, metric_name, step = False):\n",
    "    \"\"\"Extracts rows where the Metric column ends with 'metric_name' and returns only Metric, Valuem and optionally Step columns.\"\"\"\n",
    "    if step: \n",
    "        return df[df['Metric'].str.endswith(metric_name)][['Metric', \"Step\",'Value']].sort_values(by='Metric')\n",
    "    else: \n",
    "        return df[df['Metric'].str.endswith(metric_name)][['Metric', 'Value']].sort_values(by='Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bde167-8dbe-462e-b227-8ded639529c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract only a specific metric by prefix\n",
    "def extract_metric_by_prefix(df, prefix, step=False):\n",
    "    \"\"\"Extracts rows where the Metric column begins with 'prefix' and returns only Metric, Value and optionally step columns.\"\"\"\n",
    "    if step:\n",
    "        return df[df['Metric'].str.startswith(prefix)][['Metric', \"Step\", 'Value']].sort_values(by='Metric')\n",
    "    else:\n",
    "        return df[df['Metric'].str.startswith(prefix)][['Metric', 'Value']].sort_values(by='Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82811b44-c5e9-4299-85ca-6e81695d7ca8",
   "metadata": {},
   "source": [
    "# Step 3 - Define Some Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c94a1d-6082-4b8d-8699-16fbca9512ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_step_metrics(df, title='Metrics vs Step', save = True):\n",
    "    \"\"\"Plots all columns in the given DataFrame against the index with a legend.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot each column\n",
    "    for column in df.columns:\n",
    "        plt.plot(df.index, df[column], marker='o', linestyle='-', label=column)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save: \n",
    "        plt.savefig(f\"charts/{title}.png\", bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd27a4-9303-4be3-a9e7-7b64abd6da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_median_comparison(df_dict, title=\"Median_Metric_Comparison_By_Epoch_And_Run\", save = True):\n",
    "    \"\"\"Plots the 'median' column against the index for each DataFrame in the dictionary.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot each DataFrame\n",
    "    for label, df in df_dict.items():\n",
    "        plt.plot(df.index, df['median'], marker='o', linestyle='-', label=label)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Median Value')\n",
    "    plt.title(title)\n",
    "    #plt.ylim(-1, 1)  # Set fixed y-axis range\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save: \n",
    "        plt.savefig(f\"charts/{title}.png\", bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba8bb6-44b3-4d5d-9cee-72d329c6894a",
   "metadata": {},
   "source": [
    "# Step 4 Examine Test-KGE For An Example Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ffd26c-0982-4f59-9a38-e9863c3e4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = extract_metric_by_prefix(df_metrics, \"test_kge\", step=True)\n",
    "summary = df_filtered.groupby(\"Step\")[\"Value\"].agg([\"median\", \"mean\", \"min\", \"max\", \"std\"])\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817181d-c31c-4a77-8067-abafef361fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl = f\"Summary of Test_KGE Metrics on Validation Set By Epoch for run {key}\"\n",
    "plot_step_metrics(summary, title = ttl, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2896e-a392-4fd1-b2c1-85628ba32316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in on just the median and the mean\n",
    "ttl = f\"Summary of Median and Mean Test_KGE Metrics on Validation Set By Epoch for run {key}\" \n",
    "summary_slim = summary[[\"median\", \"mean\"]]\n",
    "plot_step_metrics(summary_slim, title = ttl, save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b598847e-2fc5-4b87-b37b-b9fbadf0bf3f",
   "metadata": {},
   "source": [
    "# Step 4B - Test Recur KGE for a Sample Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617a10c-f939-42c7-b85a-ae2859975989",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"Mar_DI7\"\n",
    "df_filtered_recur = extract_metric_by_prefix(df_metrics, \"test_recur_kge\", step=True)\n",
    "summary_recur = df_filtered_recur.groupby(\"Step\")[\"Value\"].agg([\"median\", \"mean\", \"min\", \"max\", \"std\"])\n",
    "summary_recur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ca56e-6144-4f5e-b29e-04ebd863318c",
   "metadata": {},
   "source": [
    "# Step 5 - Compare Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d0b5a-7861-4c8b-a9fc-f3dc5bdb6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_dict = {}\n",
    "for key in run_dict.keys(): \n",
    "    df_metrics = df_dict[key]\n",
    "    df_filtered = extract_metric_by_prefix(df_metrics, \"test_kge\", step=True)\n",
    "    summary_df = df_filtered.groupby(\"Step\")[\"Value\"].agg([\"median\"])\n",
    "    df_summary_dict[key] = summary_df\n",
    "\n",
    "    # KGE Predict Recur    \n",
    "    df_filtered_recur = extract_metric_by_prefix(df_metrics, \"test_recur_kge\", step=True)\n",
    "    summary_recur = df_filtered_recur.groupby(\"Step\")[\"Value\"].agg([\"median\", \"mean\", \"min\", \"max\", \"std\"])\n",
    "    if not summary_recur.empty:\n",
    "        df_summary_dict[key+\"+Recur\"] = summary_recur\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d44fb-0414-40a2-beb6-6754fe508a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict(df_summary_dict, excluded_keys):\n",
    "    return {k: v for k, v in df_summary_dict.items() if k not in excluded_keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceeba82-6464-47c9-adc8-4cfc993958d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict_inc(df_summary_dict, included_keys):\n",
    "    return {k: v for k, v in df_summary_dict.items() if k in included_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b690c3-17a3-4aab-bdb7-27f006df41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_comparison(df_summary_dict, title=\"Median_Metric_Comparison_By_Epoch_And_Run\", save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5469197-3434-4075-b65a-95e2dd64a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f717e6-1bd2-4014-93f2-6c17efec823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_keys = [\"Mar_DI7\", \"Mar_DI30\", \"Mar_DI7-lowLR\"] \n",
    "sum_dict_small = filter_dict(df_summary_dict, excluded_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5b045-0a3b-4b6c-b7a5-a97f644f0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_comparison(sum_dict_small, title=\"Median_Metric_Comparison_By_Epoch_And_Run2\", save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f516472-be4b-4d2c-96f3-fb3bd70688d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_keys = [\"Mar_Base_Ultra_Low\", \"Mar_Base_Ultra_high\", \"Mar_Base\"]\n",
    "sum_dict_base = filter_dict_inc(df_summary_dict, included_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fb77c-ab1a-4431-b52f-004847c48109",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_comparison(sum_dict_base, title=\"Median_Metric_Comparison_By_Epoch_And_Run2\", save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de3a0d-1ebf-4047-97d0-cf36ba8f3161",
   "metadata": {},
   "source": [
    "# Determine max kge for all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97698a-3588-414b-9998-d75c314c2ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = list(df_summary_dict.keys())\n",
    "for key in all_keys: \n",
    "    df = df_summary_dict[key]\n",
    "    medians = df[\"median\"]\n",
    "    idx = medians.idxmax()  # Get the index of the max median value\n",
    "    highest_median = np.max(medians)\n",
    "    print(f\"for run {key} the highest median test kge achieved was {highest_median:.3f} in epoch {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6638f-0e1e-4492-beb0-9a77d0f93d58",
   "metadata": {},
   "source": [
    "# NOtes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc86cc8-01e7-49ab-9ba8-277424f115d4",
   "metadata": {},
   "source": [
    "Nothing beats Mar Base epoch 4 (.81)\n",
    "The DI runs almost match Mar base WITHOUT predict recur, and do worse with predict recur \n",
    "Conclusion is DI doesn't help much with maritime "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
