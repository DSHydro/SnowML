{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a263a94-49b4-4525-9ac3-8c40e1de4533",
   "metadata": {},
   "source": [
    "# Notebook to explore inherent model variance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6db611-f956-4448-97da-2ceb4c8e0afb",
   "metadata": {},
   "source": [
    "# Step 0 - Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41df601e-19ff-41ad-92bf-c1bb6c3cb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from snowML.datapipe.utils import data_utils as du\n",
    "from snowML.LSTM import set_hyperparams as sh\n",
    "from snowML.LSTM import LSTM_pre_process as pp \n",
    "from snowML.LSTM import LSTM_train as LSTM_tr\n",
    "from snowML.LSTM import LSTM_metrics as met\n",
    "from snowML.LSTM import LSTM_plot3 as plot3\n",
    "from snowML.Scripts import local_training_mixed_loss as ml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081965c8-5ce1-4263-b1f1-03d1e1771592",
   "metadata": {},
   "source": [
    "# Step 1 - Define HyperParams and Test Huc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a9570d-eb32-4965-a1c5-80747098e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc = '170200090101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d149ff18-f175-4fb9-82c3-0164ee14565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparams\n",
    "# load base line params \n",
    "params = sh.create_hyper_dict()\n",
    "# reset the ones we care about\n",
    "params[\"learning_rate\"] = .001\n",
    "params[\"n_epochs\"] = 10    \n",
    "params[\"batch_size\"] = 32\n",
    "params[\"var_list\"] = ['mean_pr', 'mean_tair']\n",
    "params[\"expirement_name\"] = \"MultipleRunsSameHuc\"\n",
    "params[\"loss_type\"] = \"mse\"\n",
    "params[\"train_size_dimension\"] = \"time\"\n",
    "params[\"train_size_fraction\"] = .67\n",
    "params[\"recursive_predict\"] = False \n",
    "params[\"UCLA\"] = False # start w/ UA data then update \n",
    "#params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e94f9e-c8cc-4fc5-bec6-6e65e2e91bcf",
   "metadata": {},
   "source": [
    "# Step 2 - Define model functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd59871-6c73-476c-9fed-4080c8026866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(huc, params): \n",
    "    # normalize the data and create train/test split \n",
    "    df_dict = pp.pre_process_separate([huc], params[\"var_list\"], UCLA = params[\"UCLA\"], filter_dates=params[\"filter_dates\"])\n",
    "    train_size_frac = params[\"train_size_fraction\"]\n",
    "    df = df_dict[huc]\n",
    "    df_train, _, _, _ = pp.train_test_split_time(df, train_size_frac)\n",
    "    return df_dict, df_train\n",
    "\n",
    "def train_model (df_train, params): \n",
    "    model_dawgs, optimizer_dawgs, loss_fn_dawgs = ml.initialize_model(params)\n",
    "   \n",
    "    for epoch in range(params[\"n_epochs\"]):\n",
    "        # for local training, call fine_tune instead of pre_train\n",
    "        #print(f\"Training in epoch {epoch}\")\n",
    "        LSTM_tr.fine_tune(\n",
    "            model_dawgs,\n",
    "            optimizer_dawgs,\n",
    "            loss_fn_dawgs,\n",
    "            df_train,\n",
    "            params,\n",
    "            epoch\n",
    "            )\n",
    "    return model_dawgs \n",
    "\n",
    "def evaluate(model_dawgs_trained, df_dict, huc, params):\n",
    "    if params[\"UCLA\"]:\n",
    "        suffix = \"UCLA\"\n",
    "    else: \n",
    "        suffix = \"UA\"\n",
    "    data, y_tr_pred, y_te_pred, y_tr_true, y_te_true,  y_te_pred_recur, train_size, = LSTM_tr.predict_prep (model_dawgs_trained,\n",
    "                df_dict, huc, params)\n",
    "    metric_dict_test = met.calc_metrics(y_te_true, y_te_pred, metric_type = f\"test_{suffix}\")\n",
    "    if y_te_pred_recur is not None:\n",
    "        metric_dict_test_recur = met.calc_metrics(y_te_true, y_te_pred_recur, metric_type = f\"test_recur_{suffix}\")\n",
    "        combined_dict = {**metric_dict_test, **metric_dict_test_recur}\n",
    "    else:\n",
    "        combined_dict = metric_dict_test\n",
    "        \n",
    "    return combined_dict, data, y_tr_pred, y_te_pred, y_tr_true, y_te_true, y_te_pred_recur, train_size\n",
    "\n",
    "\n",
    "def combine_results(data, y_tr_pred, y_te_pred, y_tr_true, y_te_true, y_te_pred_recur, train_size, params):\n",
    "    results_df = data[[\"mean_swe\"]].copy()\n",
    "    results_df[\"y_tr_pred\"] = list(y_tr_pred) + [float('nan')] * (len(results_df) - len(y_tr_pred))\n",
    "    results_df[\"y_te_pred\"] = [float('nan')] * (train_size+params[\"lookback\"]) + list(y_te_pred)\n",
    "    results_df[\"y_tr_true\"] = list(y_tr_true) + [float('nan')] * (len(results_df) - len(y_tr_true))\n",
    "    results_df[\"y_te_true\"] = [float('nan')] *(train_size+params[\"lookback\"]) + list(y_te_true)\n",
    "    return results_df\n",
    "\n",
    "def label_results(df, suffix):\n",
    "    df = df.add_suffix(f\"_{suffix}\")\n",
    "    return df\n",
    "\n",
    "def run_one(huc, params, data_type = \"UA\"): \n",
    "    if data_type == \"UCLA\": \n",
    "        params[\"UCLA\"] = True \n",
    "        suffix = \"UCLA\" \n",
    "    else: \n",
    "        params[\"UCLA\"] = False\n",
    "        suffix = \"UA\"\n",
    "    df_dict, df_train = pre_process(huc, params)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning) # ignore warning about drop param being irrelevant with single deep layer\n",
    "        model_dawgs_trained = train_model(df_train, params)\n",
    "    combined_dict, data, y_tr_pred, y_te_pred, y_tr_true, y_te_true, y_te_pred_recur, tr_size = evaluate(model_dawgs_trained, df_dict, huc, params)\n",
    "    #df_results = combine_results(data, y_tr_pred, y_te_pred, y_tr_true, y_te_true, y_te_pred_recur, tr_size, params)\n",
    "    #df_results = label_results(df_results, suffix)\n",
    "    #return combined_dict, df_results, tr_size\n",
    "    return combined_dict\n",
    "\n",
    "def dict_to_single_row_df(data_dict):\n",
    "    \"\"\"\n",
    "    Transforms a dictionary into a pandas DataFrame with one row.\n",
    "    \n",
    "    Parameters:\n",
    "        data_dict (dict): The dictionary to transform.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with one row and keys as column names.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame([data_dict])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad2749-60ac-4a59-a1dd-7cac337799b2",
   "metadata": {},
   "source": [
    "# Step 3 - Get Results Sample Huc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3806a53-cc0f-4435-b25c-6c9f866020c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc = '170200090101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e2ef67-5b78-49b8-8d29-04978cf1a9b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mdict\u001b[39m, df, _ \u001b[38;5;241m=\u001b[39m run_one(huc, params)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "dict, df, _ = run_one(huc, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62f388-2d51-4861-a5e7-0356ba20da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72acd3e-0ffe-45af-8aed-18fa73fa590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55c8c6-dc2a-4adc-8901-6a10ccaa70e5",
   "metadata": {},
   "source": [
    "# Step 4 Loop through all the tum hucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8715950-f071-4d4a-bdc0-a74718e127ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = None\n",
    "count = 0\n",
    "\n",
    "for huc in hucs_tum:    \n",
    "    count += 1\n",
    "    print(f\"processing huc {count}\")\n",
    "    dict, df = run_all(huc, params)\n",
    "    f_out = f\"model_results_UA_UCLA_huc_{huc}\"\n",
    "    b = \"snowml-results\"\n",
    "    du.dat_to_s3(df, b, f_out, file_type=\"csv\")\n",
    "    new_row = pd.DataFrame([dict])\n",
    "    new_row[\"huc\"] = huc\n",
    "    \n",
    "    if df_results is None:\n",
    "        df_results = new_row\n",
    "    else:\n",
    "        df_results = pd.concat([df_results, new_row], ignore_index=True)\n",
    "\n",
    "df_results.set_index(\"huc\", inplace=True)\n",
    "f_out = \"metrics_UA_v_UCLA_maritime\"\n",
    "du.dat_to_s3(df_results, b, f_out, file_type=\"csv\")\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b838d9-06c7-4748-8786-67e6db3f5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"kge_diff\"] = df_results[\"test_UA_kge\"] - df_results[\"test_UCLA_kge\"]\n",
    "df_results.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1a943-0e52-48ea-8b7a-461db9c6d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7d499-053c-413d-9d7c-d43883bffb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df_results.sort_values(by=\"kge_diff\", ascending=False)\n",
    "df_sorted.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9c609-47d7-461f-8745-c75e125d52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run():\n",
    "        #ml.set_ml_server(params)\n",
    "        # log all the params\n",
    "        #mlflow.log_params(params)\n",
    "        # log the hucs & train size fraction\n",
    "        #mlflow.log_param(\"hucs\", hucs)\n",
    "        # log the model\n",
    "        #mlflow.pytorch.log_model(model_dawgs, artifact_path=f\"model_{huc}\", pickle_module=cloudpickle)\n",
    "        #mlflow.pytorch.log_model(model_dawgs, artifact_path=f\"model_{huc}\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
