{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXoOmP2oEHl3"
   },
   "source": [
    "# **Tutorial** - Time Series Prediction of Snow Water Equivalent (SWE) Using LSTM in PyTorch - Now Using UA SWE Data & Hybrid Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a slightly modified version of the protoype model provided to the Frosty Dawgs team, demonstrating an LSTM model to predict SWE on several Huc10 units in Skagit Basin.  It represents the starting point for the team's work and we thank the authors of the original prototype model.  \n",
    "\n",
    "This notebook modifies the original noebook by chainging the data source to use a longer time series of data (UA data).  Additional modifications include: (1) adds a KGE goodness of fit metric, (2) adds ML Flow tracking capabilities and (3) normalizes the UA data prior to model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX11raceEEzh"
   },
   "source": [
    "First, we import all the necessary libraries such as `torch`, `numpy`, `pandas`, and others for data preprocessing, model building, and evaluation. These libraries are key for handling data, neural networks, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpEv280r4LPs",
    "outputId": "47e86071-eb30-4695-ba19-67b926dd1c5d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gdown\n",
    "import torch\n",
    "import warnings\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "from snowML import data_utils as du\n",
    "\n",
    "from torch import nn\n",
    "#from tqdm.autonotebook import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6KcXG_ptSHA"
   },
   "source": [
    "##  Set the MLflow tracking server\n",
    "\n",
    "Note: Assumes you have already started mlflow by opening a terminal withy mlflow installed and running mlflow uo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "meXh6UVotPcF",
    "outputId": "f9a002d4-24e6-4493-95c4-d3420e9b48dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://sues-test/199', creation_time=1740199897768, experiment_id='199', last_update_time=1740199897768, lifecycle_stage='active', name='ProtoType_Results', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set our tracking server uri for logging\n",
    "tracking_uri = \"arn:aws:sagemaker:us-west-2:677276086662:mlflow-tracking-server/dawgsML\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "# Create a new MLflow Experiment called \"LSTM\"\n",
    "mlflow.set_experiment(\"ProtoType_Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFoVfo5-EaUQ"
   },
   "source": [
    "## Data Prepration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads the Dawgs model ready data for a given huc number\n",
    "\n",
    "def get_dogs_data(huc_list, var_list):\n",
    "    bucket_name = \"snowml-model-ready\"\n",
    "    df_dict = {}  # Initialize dictionary\n",
    "    for huc in huc_list: \n",
    "        file_name = f\"model_ready_huc{huc}.csv\"\n",
    "        df = du.s3_to_df(file_name, bucket_name)\n",
    "        df['day'] = pd.to_datetime(df['day'])\n",
    "        df.set_index('day', inplace=True)  # Set 'day' as the index\n",
    "        df = df[var_list]\n",
    "        df_dict[huc] = df  # Store DataFrame in dictionary\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_pr</th>\n",
       "      <th>mean_tair</th>\n",
       "      <th>mean_swe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1983-10-01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.683</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-10-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.416</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean_pr  mean_tair  mean_swe\n",
       "day                                     \n",
       "1983-10-01      0.0      4.683       0.0\n",
       "1983-10-02      0.0      5.416       0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huc_list = [1711000504, 1711000505, 1711000506, 1711000507, 1711000508, 1711000509, 1711000511]\n",
    "var_list = [\"mean_pr\", \"mean_tair\", \"mean_swe\"]\n",
    "df_dict = get_dogs_data(huc_list, var_list)\n",
    "# display an example \n",
    "df = df_dict[1711000506]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "giNglL3OF5xh"
   },
   "outputs": [],
   "source": [
    "# This function normalizes the data using the Z-score formula, which helps to standardize the features\n",
    "\n",
    "def z_score_normalize(df):\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    for column in [\"mean_pr\", \"mean_tair\"]:\n",
    "        column_mean = df[column].mean()\n",
    "        column_std = df[column].std()\n",
    "        normalized_df[column] = (df[column] - column_mean) / column_std\n",
    "\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2QhMko3Eppc"
   },
   "source": [
    "## **Creating Dataset for Time Series Prediction**\n",
    "\n",
    "This function transforms time-series data into a format suitable for model training. It uses the lookback parameter to determine how many previous time steps to consider as input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UfRipIqnGBJ-"
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: A pandas DataFrame of time series data\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - lookback):\n",
    "        feature = dataset.iloc[i:(i + lookback), :2].values  # Select first two columns\n",
    "        target = dataset.iloc[i + lookback, -1:].values  # Selects the last column dynamically\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_size_fraction):\n",
    "    train_size_main = int(len(data) * train_size_fraction)\n",
    "    test_size_main = len(data) - train_size_main\n",
    "    train_main, test_main = data[:train_size_main], data[train_size_main:]\n",
    "    return train_main, test_main, train_size_main, test_size_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv7SEozFE_Iw"
   },
   "source": [
    "## **Snow Model (LSTM Neural Network)**\n",
    "\n",
    "This is a simple LSTM-based neural network model designed for predicting mean_swe values. The model uses one LSTM layer followed by a linear layer and LeakyReLU activation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jx8jdPyXGDFO"
   },
   "outputs": [],
   "source": [
    "class SnowModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class, num_layers, dropout):\n",
    "        super(SnowModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers, dropout=self.dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_class)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        hidden_states = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        cell_states = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm1(x, (hidden_states, cell_states))\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        out = self.leaky_relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGE_Loss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(KGE_Loss, self).__init__()\n",
    "        self.eps = eps  # Small constant for numerical stability\n",
    "\n",
    "    def forward(self, pred, obs):\n",
    "        # Ensure tensors are at least 1D\n",
    "        if pred.ndim == 0 or obs.ndim == 0:\n",
    "            return torch.tensor(float(\"nan\"), device=pred.device)\n",
    "\n",
    "        obs_mean = torch.mean(obs)\n",
    "        pred_mean = torch.mean(pred)\n",
    "\n",
    "        obs_std = torch.std(obs) + self.eps  # Avoid division by zero\n",
    "        pred_std = torch.std(pred) + self.eps\n",
    "\n",
    "        # Compute Pearson correlation manually to avoid `torch.corrcoef` issues\n",
    "        covariance = torch.mean((pred - pred_mean) * (obs - obs_mean))\n",
    "        r = covariance / (obs_std * pred_std + self.eps)  # Avoid zero denominator\n",
    "\n",
    "        # Ensure r is within valid range for correlation (due to numerical errors)\n",
    "        r = torch.clamp(r, -1 + self.eps, 1 - self.eps)\n",
    "\n",
    "        alpha = pred_std / obs_std  # Standard deviation ratio\n",
    "        beta = pred_mean / (obs_mean + self.eps)  # Mean ratio\n",
    "\n",
    "        # Compute KGE\n",
    "        kge = 1 - torch.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "\n",
    "        # Ensure loss is finite\n",
    "        if torch.isnan(kge) or torch.isinf(kge):\n",
    "            return torch.tensor(float(\"nan\"), device=pred.device)\n",
    "\n",
    "        return -kge  # Negative KGE for loss minimization\n",
    "\n",
    "\n",
    "# loss_fn_snotel = KGE_Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_KGE_MSE_Loss(nn.Module):\n",
    "    def __init__(self, initial_lambda=1.0, final_lambda=1.0, total_epochs=30, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Hybrid loss combining -KGE and MSE.\n",
    "        :param initial_lambda: Initial weight for MSE loss (higher at the start).\n",
    "        :param final_lambda: Final weight for MSE loss (lower in later epochs).\n",
    "        :param total_epochs: Total training epochs for lambda scheduling.\n",
    "        :param eps: Small constant for numerical stability.\n",
    "        \"\"\"\n",
    "        super(Hybrid_KGE_MSE_Loss, self).__init__()\n",
    "        self.initial_lambda = initial_lambda\n",
    "        self.final_lambda = final_lambda\n",
    "        self.total_epochs = total_epochs\n",
    "        self.current_epoch = 0\n",
    "        self.eps = eps\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        \"\"\"Update lambda dynamically per epoch.\"\"\"\n",
    "        self.current_epoch = epoch\n",
    "        progress = epoch / self.total_epochs\n",
    "        self.lambda_mse = self.initial_lambda * (1 - progress) + self.final_lambda * progress  # Linear decay\n",
    "\n",
    "    def forward(self, pred, obs):\n",
    "        # Ensure tensors are at least 1D\n",
    "        if pred.ndim == 0 or obs.ndim == 0:\n",
    "            return torch.tensor(float(\"nan\"), device=pred.device)\n",
    "\n",
    "        obs_mean = torch.mean(obs)\n",
    "        pred_mean = torch.mean(pred)\n",
    "\n",
    "        obs_std = torch.std(obs) + self.eps  # Avoid division by zero\n",
    "        pred_std = torch.std(pred) + self.eps\n",
    "\n",
    "        # Compute Pearson correlation manually\n",
    "        covariance = torch.mean((pred - pred_mean) * (obs - obs_mean))\n",
    "        r = covariance / (obs_std * pred_std + self.eps)  # Avoid zero denominator\n",
    "\n",
    "        # Clamp r within [-1, 1] to prevent invalid values\n",
    "        r = torch.clamp(r, -1 + self.eps, 1 - self.eps)\n",
    "\n",
    "        alpha = pred_std / obs_std\n",
    "        beta = pred_mean / (obs_mean + self.eps)\n",
    "\n",
    "        # Compute KGE\n",
    "        kge = 1 - torch.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "\n",
    "        # Hybrid loss: -KGE + lambda * MSE\n",
    "        mse = self.mse_loss(pred, obs)\n",
    "        hybrid_loss = -kge + self.lambda_mse * mse\n",
    "\n",
    "        return hybrid_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPU_2MkPFDuR"
   },
   "source": [
    "## **Training the Model**\n",
    "\n",
    "This function trains the model on the training data for a specified number of epochs and batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-ygh0YurGGNk"
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_fn, X_train, y_train, n_epochs, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in  range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        loss_fn.set_epoch(epoch)\n",
    "        model.train()\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        # Validation\n",
    "        #if epoch % 10 != 0:\n",
    "            #continue\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_train)\n",
    "            train_loss = loss_fn(y_pred, y_train)\n",
    "            print(f\"Epoch {epoch}: train loss {train_loss:.4f}\")\n",
    "\n",
    "        epoch_losses.append(epoch_loss / len(loader))\n",
    "\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU1qsT1fFJGb"
   },
   "source": [
    "## **Predicting and Plotting Results**\n",
    "\n",
    "This function predicts values on the training and test datasets and visualizes the predictions compared to the actual mean_swe values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fpTDgqWkGKkh"
   },
   "outputs": [],
   "source": [
    "def predict(data, model, X_train,X_test, lookback, train_size, huc_no):\n",
    "    data = data.astype(object)\n",
    "    with torch.no_grad():\n",
    "        train_plot = np.full_like(data['mean_swe'].values, np.nan, dtype=float)\n",
    "        y_pred = model(X_train)\n",
    "        print(y_pred.shape)\n",
    "        y_pred_new = y_pred[:,  -1].unsqueeze(1)\n",
    "        print(y_pred_new.shape)\n",
    "        print(type(lookback),type(train_size))\n",
    "        train_plot[lookback:train_size] = y_pred_new.numpy().flatten()\n",
    "\n",
    "        # shift test predictions for plotting\n",
    "        test_plot = np.full_like(data['mean_swe'].values, np.nan, dtype=float)\n",
    "        test_plot[train_size+lookback:len(data)] = model(X_test)[:,  -1].unsqueeze(1).numpy().flatten()\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))  # Create figure explicitly\n",
    "    ax.plot(data.index, data['mean_swe'], c='b', label='Actual')\n",
    "    ax.plot(data.index, train_plot, c='r', label='Train Predictions')\n",
    "    ax.plot(data.index[train_size+lookback:], test_plot[train_size+lookback:], c='g', label='Test Predictions')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('mean_swe')\n",
    "    ttl = f\"UAData_SWE_Post_Predictions_for_huc_{huc_no}\"\n",
    "    ax.set_title(ttl)\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(f\"{ttl}.png\", bbox_inches='tight')\n",
    "    mlflow.log_artifact(f\"{ttl}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9f0_Uv-FSB-"
   },
   "source": [
    "## **Model Evaluation**\n",
    "This function evaluates the model using standard metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared score (R2). In addition, Kling-gupta-efficiency (KGE) added by the Frosty Dawgs team. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kling_gupta_efficiency(y_true, y_pred):\n",
    "    r = np.corrcoef(y_true.ravel(), y_pred.ravel())[0, 1] # Correlation coefficient\n",
    "    alpha = np.std(y_pred) / np.std(y_true)  # Variability ratio\n",
    "    beta = np.mean(y_pred) / np.mean(y_true)  # Bias ratio\n",
    "    kge = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "    #print(f\"r: {r}, alpha: {alpha}, beta: {beta}\")\n",
    "    return kge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fVbyPyfdHM_8"
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, X_train, y_train, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(X_train)\n",
    "        y_test_pred = model(X_test)\n",
    "\n",
    "    train_mse = mean_squared_error(y_train.numpy(), y_train_pred.numpy())\n",
    "    test_mse = mean_squared_error(y_test.numpy(), y_test_pred.numpy())\n",
    "    train_mae = mean_absolute_error(y_train.numpy(), y_train_pred.numpy())\n",
    "    test_mae = mean_absolute_error(y_test.numpy(), y_test_pred.numpy())\n",
    "    train_r2 = r2_score(y_train.numpy(), y_train_pred.numpy())\n",
    "    test_r2 = r2_score(y_test.numpy(), y_test_pred.numpy())\n",
    "\n",
    "    test_kge = kling_gupta_efficiency(y_test.numpy(), y_test_pred.numpy())  \n",
    "    train_kge = kling_gupta_efficiency(y_train.numpy(), y_train_pred.numpy())  \n",
    "\n",
    "    return [train_mse, test_mse, train_mae, test_mae, train_r2, test_r2, train_kge, test_kge]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "C6oA7LyXHQGd"
   },
   "outputs": [],
   "source": [
    "def  get_csv_filenames(directory):\n",
    "  \"\"\" Returns a sorted list of CSV filenames from the given directory.\"\"\"\n",
    "  return  sorted([f for f in os.listdir(directory)  if f.endswith('.csv')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHb8t2oBFZ2g"
   },
   "source": [
    "## **Running the Pipeline**\n",
    "\n",
    "This section retrieves the filenames from the dataset directories, filters and merges data, and finally trains and evaluates the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OTl5FU3cHQ32",
    "outputId": "61191c3f-48ab-4ebe-ff7e-5a6fd62c8753",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA datais mean_pr      0\n",
      "mean_tair    0\n",
      "mean_swe     0\n",
      "dtype: int64\n",
      "\n",
      "Lookback: 180\n",
      "Epoch 0: train loss -0.5808\n",
      "Epoch 1: train loss -0.6560\n",
      "Epoch 2: train loss -0.8843\n",
      "Epoch 3: train loss -0.9086\n",
      "Epoch 4: train loss -0.6413\n",
      "Epoch 5: train loss -0.8509\n",
      "Epoch 6: train loss -0.9226\n",
      "Epoch 7: train loss -0.8943\n",
      "Epoch 8: train loss -0.9573\n",
      "Epoch 9: train loss -0.9188\n",
      "Epoch 10: train loss -0.8084\n",
      "Epoch 11: train loss -0.9713\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters\n",
    "input_size=2\n",
    "hidden_size=2**6\n",
    "num_class=1\n",
    "num_layers=1\n",
    "dropout = 0.5\n",
    "\n",
    "learning_rate = 1e-3 #3e-3\n",
    "n_epochs = 30\n",
    "train_size_fraction = 0.67\n",
    "lookback_values =  [180]\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # log all the params\n",
    "    mlflow.log_param(\"Training From\", \"Prototype Notebook\")\n",
    "    mlflow.log_param(\"Data Source\", \"UA SWE Data\")\n",
    "    mlflow.log_param(\"input_size\", input_size)\n",
    "    mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "    mlflow.log_param(\"num_class\", num_class)\n",
    "    mlflow.log_param(\"num_layers\", num_layers)\n",
    "    mlflow.log_param(\"dropout\", dropout)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"train_size_fraction\", train_size_fraction)\n",
    "    mlflow.log_param(\"lookback_values\", lookback_values)\n",
    "    mlflow.log_param(\"epochs\", n_epochs) \n",
    "    \n",
    "\n",
    "    model_snotel = SnowModel(input_size, hidden_size, num_class, num_layers, dropout)\n",
    "    optimizer_snotel = optim.Adam(model_snotel.parameters())\n",
    "    #loss_fn_snotel = nn.MSELoss()\n",
    "    #loss_fn_snotel = KGE_Loss()\n",
    "    loss_fn_snotel = Hybrid_KGE_MSE_Loss(initial_lambda=1.0, final_lambda=0.1, total_epochs=n_epochs)\n",
    "    mlflow.log_param(\"loss_function\", loss_fn_snotel)\n",
    "    \n",
    "\n",
    "    for huc_no in huc_list: \n",
    "        # Get the model ready data \n",
    "        data = df_dict[(huc_no)]\n",
    "        data = z_score_normalize(data)\n",
    "        print(f\"NA datais {data.isna().sum()}\")\n",
    "        data = data.fillna(method='bfill')\n",
    "        train_main, test_main, train_size_main, test_size_main = train_test_split(data, train_size_fraction)\n",
    "\n",
    "        # Usage example\n",
    "        lookback_values =  [180]\n",
    "        results = []\n",
    "\n",
    "        for lookback in lookback_values:\n",
    "            print(f\"\\nLookback: {lookback}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Snotel dataset\n",
    "            X_train_snotel, y_train_snotel = create_dataset(train_main, lookback)\n",
    "            X_test_snotel, y_test_snotel = create_dataset(test_main, lookback)\n",
    "\n",
    "            train_model(model_snotel, optimizer_snotel, loss_fn_snotel, X_train_snotel, y_train_snotel, n_epochs=n_epochs, batch_size=8) \n",
    "            predict(data,model_snotel,  X_train_snotel,X_test_snotel, lookback, train_size_main, huc_no)\n",
    "            snotel_metrics = evaluate_metrics(model_snotel, X_train_snotel, y_train_snotel, X_test_snotel, y_test_snotel)\n",
    "            print(snotel_metrics)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            results.append([lookback, 'UA', 'train_mse', snotel_metrics[0], elapsed_time])\n",
    "            results.append([lookback, 'UA', 'test_mse', snotel_metrics[1], elapsed_time])\n",
    "            results.append([lookback, 'UA', 'train_mae', snotel_metrics[2], elapsed_time])\n",
    "            results.append([lookback, 'UA', 'test_mae', snotel_metrics[3], elapsed_time])\n",
    "            results.append([lookback, 'UA', 'train_r2', snotel_metrics[4], elapsed_time])\n",
    "            results.append([lookback, 'UA', 'test_r2', snotel_metrics[5], elapsed_time])\n",
    "            results.append([lookback, 'UA', 'train_kge', snotel_metrics[6], elapsed_time])\n",
    "            results.append([lookback, 'UA', 'test_kge', snotel_metrics[7], elapsed_time])\n",
    "\n",
    "            \n",
    "            mlflow.log_metric(f\"{huc_no}_train_mse\", snotel_metrics[0])\n",
    "            mlflow.log_metric(f\"{huc_no}_test_mse\", snotel_metrics[1])\n",
    "            mlflow.log_metric(f\"{huc_no}_train_kge\", snotel_metrics[6])\n",
    "            mlflow.log_metric(f\"{huc_no}_test_kge\", snotel_metrics[7])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Create a DataFrame for the results\n",
    "        df_results = pd.DataFrame(results, columns=['Lookback', 'Dataset', 'Metric', 'Value', 'Time Taken (s)'])\n",
    "\n",
    "        # Print the DataFrame\n",
    "        print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF_rDY_1EDfl"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHAz5A8CIli2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
