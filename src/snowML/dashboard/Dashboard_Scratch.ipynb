{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f32114-59c2-4e3d-a6ea-e70e4725e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3f1492-cc65-45cf-9965-3ff250f568aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "import json\n",
    "import boto3\n",
    "from snowML.datapipe.utils import get_geos as gg\n",
    "from snowML.datapipe.utils import data_utils as du\n",
    "from snowML.datapipe import to_model_ready as mr\n",
    "from snowML.LSTM import LSTM_evaluate as evaluate \n",
    "from snowML.LSTM import LSTM_pre_process as pp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf7d71c-c064-4fc5-b897-a491e718e775",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test Logic for Getting/Plotting SWE Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6627cfb-7c08-4047-b1f3-744789ad67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(huc):\n",
    "\n",
    "    # Get UA Model Ready Data\n",
    "    f_UA = f\"model_ready_huc{huc}.csv\"\n",
    "    bucket_name = \"snowml-model-ready\"\n",
    "    if not du.isin_s3(bucket_name, f_UA):\n",
    "        df_UA =  None\n",
    "    else:\n",
    "        df_UA = du.s3_to_df(f_UA, bucket_name)\n",
    "        df_UA.set_index(\"day\", inplace=True)\n",
    "\n",
    "    # Get UCLA Model Ready Data\n",
    "    f_UCLA = f\"model_ready_huc{huc}_ucla.csv\"\n",
    "    if not du.isin_s3(bucket_name, f_UCLA):\n",
    "        df_UCLA = None\n",
    "    else:\n",
    "        df_UCLA = du.s3_to_df(f_UCLA, bucket_name)\n",
    "        df_UCLA.set_index(\"day\", inplace=True)\n",
    "\n",
    "    return df_UA, df_UCLA\n",
    "\n",
    "def slim_df(df): \n",
    "    df_slim = df[[\"mean_swe\"]]\n",
    "    return df_slim\n",
    "\n",
    "def make_plot_df(df_UA, df_UCLA): \n",
    "    df_UA_slim = slim_df(df_UA)\n",
    "    if df_UCLA is None: \n",
    "        return df_UA_slim\n",
    "    df_UCLA_slim = slim_df(df_UCLA)\n",
    "    df_UCLA_slim = df_UCLA_slim.rename(columns={\"mean_swe\": \"mean_swe_UCLA\"})\n",
    "    df_joined = df_UA_slim.join(df_UCLA_slim, how=\"inner\")\n",
    "    return df_joined\n",
    "\n",
    "def plot_swe(df_UA, df_UCLA, huc):\n",
    "    \n",
    "    df_joined =make_plot_df(df_UA, df_UCLA)\n",
    "    \n",
    "    # Ensure 'day' is datetime\n",
    "    df_joined.index = pd.to_datetime(df_joined.index, errors=\"coerce\")\n",
    "\n",
    "    # Create figure and plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(df_joined.index, df_joined[\"mean_swe\"], c='b', label=\"Mean_SWE_UAData\")\n",
    "    if df_UCLA is not None: \n",
    "        ax.plot(df_joined.index, df_joined[\"mean_swe_UCLA\"], c = \"black\", label = \"Mean_SWE_UCLA_Data\")\n",
    "    ax.set_ylim(0, 2)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('SWE (m)')\n",
    "    ax.set_title(f\"Mean SWE for HUC {huc}\")\n",
    "\n",
    "\n",
    "    # Format x-axis to show only years\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"center\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3580a8-f24a-4fb9-9a6b-78442f4d350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12 = 171100050703\n",
    "df_UA, df_UCLA = get_data(huc12)\n",
    "fig = plot_swe(df_UA, df_UCLA, huc12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9cf3be-b30d-4ea8-9fe3-04cffaa48532",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PreCalculate Geos Files for Huc8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269dcaf9-180e-4cf9-a931-61f12dbfb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = \"snowml-dashboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca657f2-2d75-44fa-a283-15b5028f0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Huc 8 in Region 17 \n",
    "R17 = gg.get_geos_with_name(17, '08')\n",
    "R17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb1adc-a50b-4af8-b769-f8088ea02a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "R17[\"Display name\"] = R17[\"huc_name\"].str.slice(0, 20) + \" (\" + R17[\"huc_id\"].astype(str) + \")\"\n",
    "R17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f57a9a-1a5f-45d1-9a1d-87da4808bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "R17 = R17.drop(columns=[\"huc_name\"])\n",
    "R17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80379911-2c3b-4951-9740-f247eabe3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "R17_2 = R17.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa309b-e0d6-4248-bea3-a1911f9edb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R17_2[\"Model Ready\"] = \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eefe617-6b5d-47d0-a755-9863fda8423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to S3 \n",
    "f = \"R17_huc8\"\n",
    "url = f\"s3://{b}/{f}.geo.parquet\"\n",
    "R17_2.to_parquet(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2436b2-39f1-4d22-899d-d66a7330f5ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Precalculate Huc12 geo files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca507a-cfc5-4c41-9a9d-883511a76f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc_08 = \"17010101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe9b86-fa6f-4666-a02c-e8a158aa2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_huc12_file(huc_08): \n",
    "    geos = gg.get_geos_with_name(huc_08, '12')\n",
    "    geos[\"Display name\"] = geos[\"huc_name\"].str.slice(0, 20) + \" (\" + geos[\"huc_id\"].astype(str) + \")\"\n",
    "    geos = geos.drop(columns=[\"huc_name\"])\n",
    "    return geos\n",
    "\n",
    "def save_file(geos, huc_08): \n",
    "    f = f\"{huc_08}_huc12\"\n",
    "    url = f\"s3://{b}/{f}.geo.parquet\"\n",
    "    geos.to_parquet(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03e8f4-4e1c-4a54-9941-8df3e6b0cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 0 \n",
    "#for huc_08 in R17[\"huc_id\"]:\n",
    "    #count += 1\n",
    "    #if count % 10 == 0: \n",
    "        #print(f\"step {count}\")\n",
    "    #geos = make_huc12_file(huc_08)\n",
    "    #save_file(geos, huc_08)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf827d-5deb-483f-bab7-1194fd16cfe1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Record Which HUCS Are Model Ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aea0d9-b3f7-4387-b5b9-0d1be439900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from S3\n",
    "f = \"R17_huc8\"\n",
    "url = f\"s3://{b}/{f}.geo.parquet\"\n",
    "df = gpd.read_parquet(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2aa01f-ad56-4e1b-b64b-bd864f546f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_errors():\n",
    "    file_path = \"model_ready_err.txt\"\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        return False \n",
    "    return True \n",
    "\n",
    "def process_and_record(huc): \n",
    "    f = \"R17_huc8\"\n",
    "    url = f\"s3://{b}/{f}.geo.parquet\"\n",
    "    df = gpd.read_parquet(url)\n",
    "    geos = gg.get_geos(huc, '12')\n",
    "    mha.process_multi_huc_quiet(geos)\n",
    "    if not check_errors(): \n",
    "        print(f\"no errors for huc{huc}!\")\n",
    "        df.loc[df[\"huc_id\"] == str(huc), \"Model Ready\"] = \"Yes\"\n",
    "        df.to_parquet(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6fb237-1bd1-46af-a2e1-3b8a6d869d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_record(17110005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc13fd0-5c0e-4ef3-ad2c-43bcc95b1c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"R17_huc8\"\n",
    "url = f\"s3://{b}/{f}.geo.parquet\"\n",
    "df = gpd.read_parquet(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91359222-c969-418c-9925-5b1d5b041c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"huc_id\"] == \"17110005\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6899b1-04ed-4835-a8de-3a2874007e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc_list = [\"17020009\", \"17110005\", \"17110006\", \"17110009\", \"17030002\", \"17110008\", \"17030001\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23b176-b1b4-45fb-bdaf-dd3b8d97a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "for huc in huc_list: \n",
    "    process_and_record(huc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5d090-5bf3-45a1-b734-4f93671fc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df[\"huc_id\"].isin(huc_list)]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11763273-2578-4fde-8479-2a8046770fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[\"Display name\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bbb5a-4df6-481e-90fd-0c149ce5c3a3",
   "metadata": {},
   "source": [
    "# Retreive and Save MLFLOW MODELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cc6d81-122b-4081-975d-7603efaf68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def model_from_MLflow(uri): \n",
    "    model = evaluate.load_model(uri)\n",
    "\n",
    "def model_to_s3(model, model_name, bucket_name = \"snowml-dashboard\"): \n",
    "    file_name = f\"{model_name}.pth\"\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "    s3.upload_file(file_name, bucket_name, f\"models/{file_name}\")\n",
    "    os.remove(file_name)\n",
    "    return file_name\n",
    "\n",
    "def params_to_s3(params, model_name, bucket_name = \"snowml-dashboard\"): \n",
    "    params_file = f\"{model_name}_params.json\"\n",
    "    with open(params_file, \"w\") as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "    s3.upload_file(params_file, bucket_name, \"models/params.json\")\n",
    "    os.remove(params_file)\n",
    "\n",
    "def get_norm(params): \n",
    "     _, global_means, global_stds = pp.pre_process(huc_list_all_tr, var_list)\n",
    "\n",
    "\n",
    "def save_all_model_data(model_uri, model_name, tracking_uri, run_id, bucket_name = \"models/params.json\"): \n",
    "    model = evaluate.load_model(model_name)\n",
    "    model_to_s3(model, model_name, bucket_name = bucket_name) \n",
    "    params = evaluate.get_params(tracking_uri, run_id, bucket_name = bucket_name)\n",
    "    params_to_s3(params, model_name)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af7d7d4e-342d-4cd9-b856-db05e4ee570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sues-test/298/a6c611d4c4cf410e9666796e3a8892b7/artifacts/epoch29_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1177d86922f6420f8a68dda0fc806b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/06 15:10:59 WARNING mlflow.pytorch: Stored model version '2.4.1.post100' does not match installed PyTorch version '2.5.1+cu124'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnowModel(\n",
      "  (lstm1): LSTM(3, 64, batch_first=True, dropout=0.5)\n",
      "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Model To USe for MultiTraining \n",
    "multi_huc_model_uri = \"s3://sues-test/298/a6c611d4c4cf410e9666796e3a8892b7/artifacts/epoch9_model/data\"\n",
    "model_name = \"Multi_Huc_Trained\"\n",
    "model = evaluate.load_model(multi_huc_model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72ce2331-37eb-4cb5-91e5-731ab707b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multi_Huc_Trained.pth'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save weights and parms to S3 \n",
    "model_to_s3(model, model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178e55d-8505-436b-ad63-286b7b0e9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Params from ML flow \n",
    "tracking_uri = \"arn:aws:sagemaker:us-west-2:677276086662:mlflow-tracking-server/dawgsML\"\n",
    "run_id = \"a6c611d4c4cf410e9666796e3a8892b7\"\n",
    "params = evaluate.get_params(tracking_uri, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5e5b189-5d9c-4b96-9c91-f6cb750778a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d353f42-3c4c-4a61-907e-b88282dbbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module to Save PreTrained Model Data to S3 for Use in Dashabord\"\"\"\n",
    "# pylint: disable=C0103\n",
    "\n",
    "import os\n",
    "import json\n",
    "#import mlflow\n",
    "#import mlflow.pytorch\n",
    "import torch\n",
    "import boto3\n",
    "#from snowML.datapipe import to_model_ready as mr\n",
    "from snowML.LSTM import LSTM_evaluate as evaluate\n",
    "from snowML.LSTM import LSTM_pre_process as pp\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def model_from_MLflow(uri):\n",
    "    model = evaluate.load_model(uri)\n",
    "    return model\n",
    "\n",
    "def model_to_s3(model, model_name, bucket_name = \"snowml-dashboard\"):\n",
    "    file_name = f\"{model_name}.pth\"\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "    s3.upload_file(file_name, bucket_name, f\"models/{file_name}\")\n",
    "    os.remove(file_name)\n",
    "    return file_name\n",
    "\n",
    "def params_to_s3(params, model_name, bucket_name = \"snowml-dashboard\"):\n",
    "    params_file = f\"{model_name}_params.json\"\n",
    "    with open(params_file, \"w\") as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "    s3.upload_file(params_file, bucket_name, f\"models/{params_file}\")\n",
    "    os.remove(params_file)\n",
    "\n",
    "def norms_to_s3(g_means, g_stds, model_name, bucket_name = \"snowml-dashboard\"):\n",
    "    g_means = g_means.to_dict()\n",
    "    g_stds = g_stds.to_dict()\n",
    "    means_file = f\"{model_name}_means.json\"\n",
    "    with open(means_file, \"w\") as f:\n",
    "        json.dump(g_means, f, indent=2)\n",
    "    s3.upload_file(means_file, bucket_name, f\"models/{means_file}\")\n",
    "    os.remove(means_file)\n",
    "    std_file = means_file = f\"{model_name}_stds.json\"\n",
    "    with open(std_file, \"w\") as f:\n",
    "        json.dump(g_stds, f, indent=2)\n",
    "    s3.upload_file(std_file, bucket_name, f\"models/{std_file}\")\n",
    "    os.remove(std_file)\n",
    "    \n",
    "\n",
    "def get_norm(params):\n",
    "    huc_list_all_tr = params[\"train_hucs\"] + params[\"val_hucs\"]\n",
    "    _, global_means, global_stds = pp.pre_process(huc_list_all_tr, params[\"var_list\"])\n",
    "    return global_means, global_stds\n",
    "\n",
    "\n",
    "def save_all_model_data(model_uri, model_name, tracking_uri, run_id):\n",
    "    model = evaluate.load_model(model_uri)\n",
    "    model_to_s3(model, model_name)\n",
    "    params = evaluate.get_params(tracking_uri, run_id)\n",
    "    params_to_s3(params, model_name)\n",
    "    return params\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
